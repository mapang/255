{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "In this homework, we are going to play with Twitter data.\n",
    "\n",
    "The data is represented as rows of of [JSON](https://en.wikipedia.org/wiki/JSON#Example) strings.\n",
    "It consists of [tweets](https://dev.twitter.com/overview/api/tweets), [messages](https://dev.twitter.com/streaming/overview/messages-types), and a small amount of broken data (cannot be parsed as JSON).\n",
    "\n",
    "For this homework, we will only focus on tweets and ignore all other messages.\n",
    "\n",
    "\n",
    "## Tweets\n",
    "\n",
    "A tweet consists of many data fields. [Here is an example](https://gist.github.com/arapat/03d02c9b327e6ff3f6c3c5c602eeaf8b). You can learn all about them in the Twitter API doc. We are going to briefly introduce only the data fields that will be used in this homework.\n",
    "\n",
    "* `created_at`: Posted time of this tweet (time zone is included)\n",
    "* `id_str`: Tweet ID - we recommend using `id_str` over using `id` as Tweet IDs, becauase `id` is an integer and may bring some overflow problems.\n",
    "* `text`: Tweet content\n",
    "* `user`: A JSON object for information about the author of the tweet\n",
    "    * `id_str`: User ID\n",
    "    * `name`: User name (may contain spaces)\n",
    "    * `screen_name`: User screen name (no spaces)\n",
    "* `retweeted_status`: A JSON object for information about the retweeted tweet (i.e. this tweet is not original but retweeteed some other tweet)\n",
    "    * All data fields of a tweet except `retweeted_status`\n",
    "* `entities`: A JSON object for all entities in this tweet\n",
    "    * `hashtags`: An array for all the hashtags that are mentioned in this tweet\n",
    "    * `urls`: An array for all the URLs that are mentioned in this tweet\n",
    "\n",
    "\n",
    "## Data source\n",
    "\n",
    "All tweets are collected using the [Twitter Streaming API](https://dev.twitter.com/streaming/overview).\n",
    "\n",
    "\n",
    "## Users partition\n",
    "\n",
    "Besides the original tweets, we will provide you with a Pickle file, which contains a partition over 452,743 Twitter users. It contains a Python dictionary `{user_id: partition_id}`. The users are partitioned into 7 groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Load data to a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets data is stored on AWS S3. We have in total a little over 1 TB of tweets. We provide 10 MB of tweets for your local development. For the testing and grading on the homework server, we will use different data.\n",
    "\n",
    "## Testing on the homework server\n",
    "In the Playground, we provide input size of 4 GB. To test, read file list from `../../Data/hw6-files-4gb.txt`. More input files will be added soon. \n",
    "\n",
    "For final submission, make sure to read files list from `../../Data/hw6-files-final.txt`. Otherwise your program will receive no points.\n",
    "\n",
    "## Local test\n",
    "\n",
    "For local testing, read files list from `../../Data/hw6-files-10mb.txt`.\n",
    "Now let's see how many lines there are in the input files.\n",
    "\n",
    "1. Make RDD from the list of files in `hw6-files-10mb.txt`.\n",
    "2. Mark the RDD to be cached (so in next operation data will be loaded in memory) \n",
    "3. call the `print_count` method to print number of lines in all these files\n",
    "\n",
    "It should print\n",
    "```\n",
    "Number of elements: 2193\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_count(rdd):\n",
    "    print 'Number of elements:', rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import findspark\n",
    "findspark.init()    # Remove the findspark lines later\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "with open(\"../../Data/hw6-files-10mb.txt\",\"r\") as f:\n",
    "    line = map(lambda x: x.rstrip(), f)\n",
    "    rdd = sc.textFile(','.join(line)).cache()\n",
    "    \n",
    "    #rdd = sc.union([sc.textFile( line.strip()) for line in f]).cache()  # with this change time is 132 sec\n",
    "print_count(rdd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parse JSON strings to JSON objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has built-in support for JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "json_example = '''\n",
    "{\n",
    "    \"id\": 1,\n",
    "    \"name\": \"A green door\",\n",
    "    \"price\": 12.50,\n",
    "    \"tags\": [\"home\", \"green\"]\n",
    "}\n",
    "'''\n",
    "\n",
    "json_obj = json.loads(json_example)\n",
    "json_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken tweets and irrelevant messages\n",
    "\n",
    "The data of this assignment may contain broken tweets (invalid JSON strings). So make sure that your code is robust for such cases.\n",
    "\n",
    "In addition, some lines in the input file might not be tweets, but messages that the Twitter server sent to the developer (such as [limit notices](https://dev.twitter.com/streaming/overview/messages-types#limit_notices)). Your program should also ignore these messages.\n",
    "\n",
    "*Hint:* [Catch the ValueError](http://stackoverflow.com/questions/11294535/verify-if-a-string-is-json-in-python)\n",
    "\n",
    "\n",
    "(1) Parse raw JSON tweets to obtain valid JSON objects. From all valid tweets, construct a pair RDD of `(user_id, text)`, where `user_id` is the `id_str` data field of the `user` dictionary (read [Tweets](#Tweets) section above), `text` is the `text` data field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "def safe_parse(raw_json):\n",
    "    # your code here\n",
    "    try:\n",
    "        json_object = json.loads(raw_json)\n",
    "    except ValueError, e: # this outputs None\n",
    "        pass\n",
    "    if 'user' in json_object and 'text' in json_object and 'recipient' not in json_object:\n",
    "        return (json_object['user']['id_str'],json_object['text'].encode('utf-8'))\n",
    "        \n",
    "    \n",
    "    return None #return False\n",
    "\n",
    "\n",
    "# def safe_parse(raw_json):   # THE T/F CASE\n",
    "#     # your code here\n",
    "#     try:\n",
    "#         json_object = json.loads(raw_json)\n",
    "#         #tweets = sc.parallelize(json_object, 4)  # WHAT number should this be?\n",
    "#         #tweets = sc.read.json(json_object).rdd\n",
    "#         #tweets = sc.read.json(raw_json)\n",
    "#         #print type(tweets)\n",
    "#         #json_filtered = tweets.filter(lambda x: x['limit'])\n",
    "#         #if 'id_str' in json_object:\n",
    "        \n",
    "#             #return (json_object['user']['id_str'],json_object['text'].encode('utf-8'))#json_filtered.map(lambda x: (x['id_str'].encode('utf-8'),x['text'].encode('utf-8')))#.collect()   # should collect the output?\n",
    "        \n",
    "#     except ValueError, e: # this outputs None\n",
    "#         return None#return False\n",
    "#         #pass\n",
    "#     if 'user' in json_object and 'text' in json_object and 'recipient' not in json_object:\n",
    "#         return json_object#return True #return json_object\n",
    "        \n",
    "    \n",
    "#     return None #return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweet_tuple = rdd.filter(lambda x: safe_parse(x)).map(lambda x: json.loads(x)).map(lambda x:(x['user']['id_str'], x['text'].encode('utf-8')) ) #use this if safe_parse returns T/F\n",
    "# #print type(tweet_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Count the number of different users in all valid tweets (hint: [the `distinct()` method](https://spark.apache.org/docs/latest/programming-guide.html#transformations)).\n",
    "\n",
    "It should print\n",
    "```\n",
    "The number of unique users is: 2083\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_users_count(count):\n",
    "    print 'The number of unique users is:', count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tweet_tuple = rdd.map(lambda x: safe_parse(x)).filter(lambda x: x != None)#.cache()\n",
    "count = tweet_tuple.map(lambda x: x[0]).distinct().count() # this is correct if safe_parse returns a tuple\n",
    "#  count = tweet_tuple.keys().distinct().count()\n",
    "#count = tweet_tuple.map(lambda x: x[0]).distinct().count() #.filter(lambda x: x != None) # use this if safe_parse returns T/F\n",
    "print_users_count(count)\n",
    "# print tweet_tuple.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Number of posts from each user partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Pickle file `../../Data/users-partition.pickle`, you will get a dictionary which represents a partition over 452,743 Twitter users, `{user_id: partition_id}`. The users are partitioned into 7 groups. For example, if the dictionary is loaded into a variable named `partition`, the partition ID of the user `59458445` is `partition[\"59458445\"]`. These users are partitioned into 7 groups. The partition ID is an integer between 0-6.\n",
    "\n",
    "Note that the user partition we provide doesn't cover all users appear in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Load the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "import pickle\n",
    "filename = \"../../Data/users-partition.pickle\"\n",
    "pd=pickle.load(open(filename,'rb'))\n",
    "#print type(List.keys()[0])\n",
    "# schemaString = \"user_id partition_id\"\n",
    "\n",
    "# fields = [StructField(field_name, IntegerType(), True) for field_name in schemaString.split()]\n",
    "# schema = StructType(fields)\n",
    "#pf=sqlc.createDataFrame(List)\n",
    "#print List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Count the number of posts from each user partition\n",
    "\n",
    "Count the number of posts from group 0, 1, ..., 6, plus the number of posts from users who are not in any partition. Assign users who are not in any partition to the group 7.\n",
    "\n",
    "Put the results of this step into a pair RDD `(group_id, count)` that is sorted by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# your code here\n",
    "#group_count = {}\n",
    "#pickle_rdd = sc.parallelize(pd.keys(), pd.values())\n",
    "# for item in pd:\n",
    "#     print item[1]\n",
    "#     if(group_count[item[1]] == None):\n",
    "#         group_count[7] = group_count[7] +1\n",
    "    \n",
    "#     else:\n",
    "#         group_count[item[1]] = group_count[item[1]]+1\n",
    "#pd = k = user_id:v = group_id\n",
    "#group = tweet_tuple.map(lambda x: (pd[x[0]], 1) if x[0]  in pd else (7,1) ).reduceByKey(lambda x,y: x+y).sortByKey().collect()\n",
    "group = tweet_tuple.flatMap(lambda x: [pd[x[0]]] if x[0]  in pd else [7] ).countByValue()#.reduceByKey(lambda x,y: x+y).sortByKey().collect()\n",
    "#print  group.items()\n",
    "\n",
    "#%time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Print the post count using the `print_post_count` function we provided.\n",
    "\n",
    "It should print\n",
    "\n",
    "```\n",
    "Group 0 posted 81 tweets\n",
    "Group 1 posted 199 tweets\n",
    "Group 2 posted 45 tweets\n",
    "Group 3 posted 313 tweets\n",
    "Group 4 posted 86 tweets\n",
    "Group 5 posted 221 tweets\n",
    "Group 6 posted 400 tweets\n",
    "Group 7 posted 798 tweets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_post_count(counts):\n",
    "    for group_id, count in counts:\n",
    "        print 'Group %d posted %d tweets' % (group_id, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "print_post_count(group.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3:  Tokens that are relatively popular in each user partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this step, we are going to find tokens that are relatively popular in each user partition.\n",
    "\n",
    "We define the number of mentions of a token $t$ in a specific user partition $k$ as the number of users from the user partition $k$ that ever mentioned the token $t$ in their tweets. Note that even if some users might mention a token $t$ multiple times or in multiple tweets, a user will contribute at most 1 to the counter of the token $t$.\n",
    "\n",
    "Please make sure that the number of mentions of a token is equal to the number of users who mentioned this token but NOT the number of tweets that mentioned this token.\n",
    "\n",
    "Let $N_t^k$ be the number of mentions of the token $t$ in the user partition $k$. Let $N_t^{all} = \\sum_{i=0}^7 N_t^{i}$ be the number of total mentions of the token $t$.\n",
    "\n",
    "We define the relative popularity of a token $t$ in a user partition $k$ as the log ratio between $N_t^k$ and $N_t^{all}$, i.e. \n",
    "\n",
    "\\begin{equation}\n",
    "p_t^k = \\log \\frac{C_t^k}{C_t^{all}}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "You can compute the relative popularity by calling the function `get_rel_popularity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0) Load the tweet tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# %load happyfuntokenizing.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\n",
    "\n",
    "Julaiti Alafate:\n",
    "  I modified the regex strings to extract URLs in tweets.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Christopher Potts\"\n",
    "__copyright__ = \"Copyright 2011, Christopher Potts\"\n",
    "__credits__ = []\n",
    "__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-nc-sa/3.0/\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Christopher Potts\"\n",
    "__email__ = \"See the author's website\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "import htmlentitydefs\n",
    "\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # URLs:\n",
    "    r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # HTML tags:\n",
    "     r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False):\n",
    "        self.preserve_case = preserve_case\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        try:\n",
    "            s = unicode(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = str(s).encode('string_escape')\n",
    "            s = unicode(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        return words\n",
    "\n",
    "    def tokenize_random_tweet(self):\n",
    "        \"\"\"\n",
    "        If the twitter library is installed and a twitter connection\n",
    "        can be established, then tokenize a random tweet.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import twitter\n",
    "        except ImportError:\n",
    "            print \"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\"\n",
    "        from random import shuffle\n",
    "        api = twitter.Api()\n",
    "        tweets = api.GetPublicTimeline()\n",
    "        if tweets:\n",
    "            for tweet in tweets:\n",
    "                if tweet.user.lang == 'en':            \n",
    "                    return self.tokenize(tweet.text)\n",
    "        else:\n",
    "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "def get_rel_popularity(c_k, c_all):\n",
    "    return log(1.0 * c_k / c_all) / log(2)\n",
    "\n",
    "\n",
    "def print_tokens(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print '=' * 5 + ' ' + group_name + ' ' + '=' * 5\n",
    "    for t, n in tokens:\n",
    "        print \"%s\\t%.4f\" % (t, n)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Tokenize the tweets using the tokenizer we provided above named `tok`. Count the number of mentions for each tokens regardless of specific user group.\n",
    "\n",
    "Call `print_count` function to show how many different tokens we have.\n",
    "\n",
    "It should print\n",
    "```\n",
    "Number of elements: 8949\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time \n",
    "\n",
    "#tokens = tweet_tuple.map(lambda x: (x[0],tok.tokenize(x[1]))).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0],list(set(x[1])))).cache()#.cache()# group all tweets of a user, thre are duplicate tokens\n",
    "\n",
    "\n",
    "\n",
    "user_tweet_tok = tweet_tuple.map(lambda x: (x[0],tok.tokenize(x[1]))).reduceByKey(lambda x,y: x+y)#.cache()# group all tweets of a user, thre are duplicate tokens\n",
    "#print user_tweet_tok.take(3)\n",
    "#tokens = user_tweet_tok.map(lambda x: tok.tokenize(x[1])).flatMap(lambda x: list(set(x))) # this worked before\n",
    "#print tokens.take(2)\n",
    "\n",
    "#print user_tweet_tok.collect()\n",
    "\n",
    "#---- pairs it up as (user, user's tokens)\n",
    "tokens = user_tweet_tok.map(lambda x: (x[0],list(set(x[1])))).cache()# get rid of duplicate tokens per user and combine all users into 1 list\n",
    "            #flat_tok = tokens.flatMap(lambda x: x[1]).cache() #was token.flatMap\n",
    "#print flat_tok.take(2)\n",
    "            #distinct_tok = flat_tok.distinct()# gets distinct tokens over all users  #.reduce(lambda x, y: print_count(x)+ print_count(y))\n",
    "\n",
    "#--- alt -------- incorrect\n",
    "# flat_tok = user_tweet_tok.flatMap(lambda x: x[1])\n",
    "# distinct_tok = flat_tok.distinct()\n",
    "\n",
    "#------ orig  wokrs -------\n",
    "# tokens = user_tweet_tok.flatMap(lambda x: list(set(x[1])))# get rid of duplicate tokens per user and combine all users into 1 list\n",
    "# distinct_tok = tokens.distinct()# gets distinct tokens over all users  #.reduce(lambda x, y: print_count(x)+ print_count(y))\n",
    "\n",
    "\n",
    "\n",
    "print_count(tokens.flatMap(lambda x: x[1]).distinct())#distinct_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Tokens that are mentioned by too few users are usually not very interesting. So we want to only keep tokens that are mentioned by at least 100 users. Please filter out tokens that don't meet this requirement.\n",
    "\n",
    "Call `print_count` function to show how many different tokens we have after the filtering.\n",
    "\n",
    "Call `print_tokens` function to show top 20 most frequent tokens.\n",
    "\n",
    "It should print\n",
    "```\n",
    "Number of elements: 44\n",
    "===== overall =====\n",
    ":\t1388.0000\n",
    "rt\t1237.0000\n",
    ".\t826.0000\n",
    "…\t673.0000\n",
    "the\t623.0000\n",
    "trump\t582.0000\n",
    "to\t499.0000\n",
    ",\t489.0000\n",
    "a\t404.0000\n",
    "is\t376.0000\n",
    "in\t297.0000\n",
    "of\t292.0000\n",
    "and\t288.0000\n",
    "for\t281.0000\n",
    "!\t269.0000\n",
    "?\t210.0000\n",
    "on\t195.0000\n",
    "i\t192.0000\n",
    "you\t191.0000\n",
    "this\t190.0000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# your code here\n",
    "# orig: #tok_used_100 = tokens.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] >= 100).map(lambda (c,v): (v,c)).sortByKey(False).map(lambda (c,v): (v,c))\n",
    "\n",
    "#------orig -------\n",
    "#tok_used_100 = flat_tok.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] >= 100).cache()\n",
    "\n",
    "tok_used_100 = tokens.mapValues(lambda x: map((lambda m: (m,1)) ,x)).flatMap(lambda x: x[1]).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] >= 100)\n",
    "#print tok_used_100.take(2)#.reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] >= 100)\n",
    "            #flat_tok.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] >= 100).cache()#.sortBy(lambda x: x[1], False)#.cache()#.map(lambda (c,v): (v,c)).sortByKey(False).map(lambda (c,v): (v,c))\n",
    "#print tok_used_100.take(2)\n",
    "print_count(tok_used_100)\n",
    "print_tokens(tok_used_100.takeOrdered(20, lambda x: -x[1] ))\n",
    "#print_tokens(tok_used_100.collect()) #ORIG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) For all tokens that are mentioned by at least 100 users, compute their relative popularity in each user group. Then print the top 10 tokens with highest relative popularity in each user group. In case two tokens have same relative popularity, break the tie by printing the alphabetically smaller one.\n",
    "\n",
    "**Hint:** Let the relative popularity of a token $t$ be $p$. The order of the items will be satisfied by sorting them using (-p, t) as the key.\n",
    "\n",
    "It should print\n",
    "```\n",
    "===== group 0 =====\n",
    "...\t-3.5648\n",
    "at\t-3.5983\n",
    "hillary\t-4.0875\n",
    "i\t-4.1255\n",
    "bernie\t-4.1699\n",
    "not\t-4.2479\n",
    "https\t-4.2695\n",
    "he\t-4.2801\n",
    "in\t-4.3074\n",
    "are\t-4.3646\n",
    "\n",
    "===== group 1 =====\n",
    "#demdebate\t-2.4391\n",
    "-\t-2.6202\n",
    "&\t-2.7472\n",
    "amp\t-2.7472\n",
    "clinton\t-2.7570\n",
    ";\t-2.7980\n",
    "sanders\t-2.8838\n",
    "?\t-2.9069\n",
    "in\t-2.9664\n",
    "if\t-3.0138\n",
    "\n",
    "===== group 2 =====\n",
    "are\t-4.6865\n",
    "and\t-4.7105\n",
    "bernie\t-4.7549\n",
    "at\t-4.7682\n",
    "sanders\t-4.9542\n",
    "that\t-5.0224\n",
    "in\t-5.0444\n",
    "donald\t-5.0618\n",
    "a\t-5.0732\n",
    "#demdebate\t-5.1396\n",
    "\n",
    "===== group 3 =====\n",
    "#demdebate\t-1.3847\n",
    "bernie\t-1.8480\n",
    "sanders\t-2.1887\n",
    "of\t-2.2356\n",
    "that\t-2.3785\n",
    "the\t-2.4376\n",
    "…\t-2.4403\n",
    "clinton\t-2.4467\n",
    "hillary\t-2.4594\n",
    "be\t-2.5465\n",
    "\n",
    "===== group 4 =====\n",
    "hillary\t-3.7395\n",
    "sanders\t-3.9542\n",
    "of\t-4.0199\n",
    "clinton\t-4.0790\n",
    "at\t-4.1832\n",
    "in\t-4.2143\n",
    "a\t-4.2659\n",
    "on\t-4.2854\n",
    ".\t-4.3681\n",
    "the\t-4.4251\n",
    "\n",
    "===== group 5 =====\n",
    "cruz\t-2.3861\n",
    "he\t-2.6280\n",
    "are\t-2.7796\n",
    "will\t-2.7829\n",
    "the\t-2.8568\n",
    "is\t-2.8822\n",
    "for\t-2.9250\n",
    "that\t-2.9349\n",
    "of\t-2.9804\n",
    "this\t-2.9849\n",
    "\n",
    "===== group 6 =====\n",
    "@realdonaldtrump\t-1.1520\n",
    "cruz\t-1.4532\n",
    "https\t-1.5222\n",
    "!\t-1.5479\n",
    "not\t-1.8904\n",
    "…\t-1.9269\n",
    "will\t-2.0124\n",
    "it\t-2.0345\n",
    "this\t-2.1104\n",
    "to\t-2.1685\n",
    "\n",
    "===== group 7 =====\n",
    "donald\t-0.6422\n",
    "...\t-0.7922\n",
    "sanders\t-1.0282\n",
    "trump\t-1.1296\n",
    "bernie\t-1.2106\n",
    "-\t-1.2253\n",
    "you\t-1.2376\n",
    "clinton\t-1.2511\n",
    "if\t-1.2880\n",
    "i\t-1.2996\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grouped_users = tokens.map(lambda x: (pd[x[0]], x[1]) if x[0]  in pd else (7, x[1]) ).reduceByKey(lambda x,y: x+y).cache()#.sortByKey().collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# your code here\n",
    "#use get_rel_popularity\n",
    "#group = tweet_tuple.map(lambda x: (pd[x[0]], 1) if x[0]  in pd else (7,1) ).reduceByKey(lambda x,y: x+y).sortByKey().collect()\n",
    "# this one contains (group,(user,tok)) #grouped_users = user_tweet_tok.map(lambda x: (pd[x[0]], [x]) if x[0]  in pd else (7,x) ).reduceByKey(lambda x,y: x+y)#.sortByKey().collect()\n",
    "\n",
    "#grouped_users = user_tweet_tok.map(lambda x: (pd[x[0]], x[1]) if x[0]  in pd else (7,x) ).reduceByKey(lambda x,y: x+y)#.sortByKey().collect()\n",
    "\n",
    "\n",
    "        #grouped_users = user_tweet_tok.map(lambda x: (pd[x[0]], x[1]) if x[0]  in pd else (7,x) ).reduceByKey(lambda x,y: x+y)#.sortByKey().collect()\n",
    "#count_grouped_tok = grouped_users.map(lambda x: (x[0], (x[1].map(lambda  y: (y, 1)))))#.reduceByKey(lambda x,y: x[1]+y[1])\n",
    "#group_tok = grouped_users.countByValue()\n",
    "#print group_tok\n",
    "\n",
    "\n",
    "\n",
    "#print user_tweet_tok.countByKey()\n",
    "\n",
    "# createCombiner = (lambda x: (x,1)) # like a map to convert x[0] to int type\n",
    "# mergeValue = (lambda X, V: (X[0], X[1])) # like a map to calc avg\n",
    "# mergeCombiners = (lambda x, y: (x[0]+y[0], x[1]+y[1])) # like a reduce\n",
    "\n",
    "# group_tok_count = group_tok.combineByKey(createCombiner, mergeValue, mergeCombiners)\n",
    "# print group_tok_count.take(1)\n",
    "#filtered_100 = grouped_users.filter(lambda x: x[1] in tok_used_100[0]) # contains only tokens in top 100 # maybe need nested filter\n",
    "#count_filtered_100 = filtered_100.map()\n",
    "#print count_grouped_tok.take(1)\n",
    "\n",
    "#filtred_100 = grouped_users.reduceByKey(lambda x,y: x/y)\n",
    "\n",
    "#user_tweet_tok.filter(lambda x: x[1] in )\n",
    "\n",
    "#     createCombiner = (lambda x: (x,1)) # like a map to convert x[0] to int type\n",
    "#     mergeValue = (lambda X, V: (X[0]+ V, X[1]+1)) # like a map to calc avg\n",
    "#     mergeCombiners = (lambda x, y: (x[0]+y[0], x[1]+y[1])) # like a reduce\n",
    "    \n",
    "#     avgRDD = RDD1.mapValues(lambda x: int(x[0])).combineByKey(createCombiner, mergeValue, mergeCombiners )\n",
    "\n",
    "#print grouped_users.take(1)\n",
    "#grouped_users_count = grouped_users.countByValues().cache()\n",
    "                #grouped_users_count = grouped_users.mapValues(lambda x: map ((lambda y: (y,1) ), x)).cache()\n",
    "\n",
    "\n",
    "#print grouped_users_count.take(2)#.items()\n",
    "#group_0 = grouped_users.filter(lambda x: x[0] == 0 ).flatMap(lambda x: x[1] ).countByValue()# this returns a defaultdict obj\n",
    "\n",
    "#print grouped_users.take(1)\n",
    "#                 for i in range(8):\n",
    "#                     group_i = grouped_users_count.filter(lambda x: x[0] == i )\\\n",
    "#                             .flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y)\\\n",
    "#                             .join(tok_used_100)\\\n",
    "#                             .mapValues(lambda x: get_rel_popularity (x[0],x[1]))\\\n",
    "#                             .sortByKey(True)\\\n",
    "#                             .sortBy(lambda x: x[1], False).take(10)\n",
    "\n",
    "#                     print_tokens(group_i, i)\n",
    "\n",
    "            #grouped_users = tokens.map(lambda x: (pd[x[0]], x[1]) if x[0]  in pd else (7, x[1]) ).reduceByKey(lambda x,y: x+y)#.cache()#.sortByKey().collect()\n",
    "\n",
    "grouped_users = tokens.map(lambda x: (pd[x[0]], x[1]) if x[0]  in pd else (7, x[1]) ).reduceByKey(lambda x,y: x+y)#.cache()#.sortByKey().collect()\n",
    "\n",
    "\n",
    "\n",
    "#------- orig -------------\n",
    "# for i in range(8):\n",
    "#     group_i = grouped_users.mapValues(lambda x: map ((lambda y: (y,1) ), x)).filter(lambda x: x[0] == i ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).sortByKey(True).sortBy(lambda x: x[1], False).take(10)\n",
    "#     print_tokens(group_i, i)\n",
    "\n",
    "#-------------- new ----------------\n",
    "for i in range(8):\n",
    "    group_i = grouped_users.filter(lambda x: x[0] == i ).flatMap(lambda x: x[1] ).map(lambda x:(x,1))\\\n",
    "            .reduceByKey(lambda x,y: x+y)\\\n",
    "            .join(tok_used_100)\\\n",
    "            .mapValues(lambda x: get_rel_popularity (x[0],x[1]))\\\n",
    "            .takeOrdered(10, lambda x: x[0] and -x[1]) #alternative\n",
    "            #.sortBy(lambda x: x[1], False).take(10)\n",
    "    print_tokens(group_i, i)\n",
    "\n",
    "#------------ test  ----------------------\n",
    "# for i in range(8):\n",
    "#     group_i = grouped_users.filter(lambda x: x[0] == i ).flatMap(lambda x: x[1] ).countByValues().items()\n",
    "#             .join(tok_used_100)\\\n",
    "#             .mapValues(lambda x: get_rel_popularity (x[0],x[1]))\\\n",
    "#             .sortByKey(True)\\\n",
    "#             .sortBy(lambda x: x[1], False).take(10)\n",
    "#     print_tokens(group_i, i)\n",
    "            \n",
    "#------------ end of new ----------------            \n",
    "            \n",
    "            \n",
    "#     print group_i.collect()\n",
    "    #.keyBy(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "    \n",
    "    #keyBy(keyfunc).sortByKey(ascending, numPartitions).values()\n",
    "\n",
    "# group_1 = grouped_users_count.filter(lambda x: x[0] == 1 ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y)\n",
    "# group_2 = grouped_users_count.filter(lambda x: x[0] == 2 ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y)\n",
    "# group_3 = grouped_users_count.filter(lambda x: x[0] == 3 ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y)\n",
    "# group_4 = grouped_users_count.filter(lambda x: x[0] == 4 ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y)\n",
    "# group_5 = grouped_users_count.filter(lambda x: x[0] == 5 ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y)\n",
    "# group_6 = grouped_users_count.filter(lambda x: x[0] == 6 ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y)\n",
    "# group_7 = grouped_users_count.filter(lambda x: x[0] == 7 ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "#grouped_users_count_sum = grouped_users_count.mapValues(lambda x: )#.mapValues(lambda x: (t[0], t[1]/c[1]) for t in x if t[0] in tok_used_100)\n",
    "                                    # grouped_users_count_tot = grouped_users_count.mapValues(lambda x: ( ))\n",
    "#print group_i.collect()\n",
    "\n",
    "# .reduceByKey(add)\n",
    "#     .map(lambda kv: (kv[0][0], (kv[0][1], kv[1])))\n",
    "#     .groupByKey()\n",
    "#     .mapValues(lambda xs: (list(xs), sum(x[1] for x in xs))))\n",
    "                                    # .mapValues(lambda xs: (list(xs), sum(x[1] for x in xs))))\n",
    "#d = defaultdict(list)\n",
    "# for tag, num in my_list:\n",
    "#     d[tag].append(num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#grouped_users_count_tot = grouped_users_count.mapValues(lambda x: reduce((lambda z,y: z[1]+y[1]), x))\n",
    "        # grouped_users_count_tot = grouped_users_count.map(lambda x: x[1]).reduceByKey(lambda x,y: x+y)\n",
    "        # print grouped_users_count_tot.take(3)\n",
    "\n",
    "# grouped_users_count_tot = grouped_users_count\n",
    "# print grouped_users_count_tot.take(1)    \n",
    "#print grouped_users_count.foldByKey(0,lambda x,y: x+y).take(1)\n",
    "# grouped_users_count_tot = grouped_users.mapValues(lambda x: reduce ((lambda x,y: x[1]+y[1] ), x))\n",
    "# print grouped_users_count_tot.take(1)\n",
    "\n",
    "                            # createCombiner = (lambda x: (x,1)) # like a map to initialize all v in (k,v) to 1 -> (k,(v,1))\n",
    "                            # mergeValue = (lambda X, V: (X[0], X[1]+V)) # like a map to calc avg\n",
    "                            # mergeCombiners = (lambda x, y: (x[0]+y[0], x[1]+y[1])) # like a reduce\n",
    "                            # group_tok_count = grouped_users_count.combineByKey(createCombiner, mergeValue, mergeCombiners)\n",
    "\n",
    "                            # print group_tok_count.take(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(4) (optional, not for grading) The users partition is generated by a machine learning algorithm that tries to group the users by their political preferences. Three of the user groups are showing supports to Bernie Sanders, Ted Cruz, and Donald Trump. \n",
    "\n",
    "If your program looks okay on the local test data, you can try it on the larger input by submitting your program to the homework server. Observe the output of your program to larger input files, can you guess the partition IDs of the three groups mentioned above based on your output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the values of the following three items to your guesses\n",
    "users_support = [\n",
    "    (-1, \"Bernie Sanders\"),\n",
    "    (-1, \"Ted Cruz\"),\n",
    "    (-1, \"Donald Trump\")\n",
    "]\n",
    "\n",
    "for gid, candidate in users_support:\n",
    "    print \"Users from group %d are most likely to support %s.\" % (gid, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#!pip install cPickle\n",
    "\n",
    "# # Homework 6\n",
    "\n",
    "# Name: Michelle Pang\n",
    "# Email: mapang@eng.ucsd.edu\n",
    "# PID: A10660264\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import findspark\n",
    "findspark.init()    # Remove the findspark lines later\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"../../Data/hw6-files-10mb.txt\",\"r\") as f:\n",
    "    line = map(lambda x: x.strip(), f)\n",
    "    rdd = sc.textFile(','.join(line)).cache()\n",
    "    #rdd = sc.union([sc.textFile( line.strip()) for line in f]).cache()  # with this change time is 132 sec\n",
    "\n",
    "\n",
    "filename = \"../../Data/users-partition.pickle\"\n",
    "pd=pickle.load(open(filename,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\n",
    "\n",
    "Julaiti Alafate:\n",
    "  I modified the regex strings to extract URLs in tweets.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Christopher Potts\"\n",
    "__copyright__ = \"Copyright 2011, Christopher Potts\"\n",
    "__credits__ = []\n",
    "__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-nc-sa/3.0/\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Christopher Potts\"\n",
    "__email__ = \"See the author's website\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "import htmlentitydefs\n",
    "\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # URLs:\n",
    "    r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # HTML tags:\n",
    "     r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False):\n",
    "        self.preserve_case = preserve_case\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        try:\n",
    "            s = unicode(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = str(s).encode('string_escape')\n",
    "            s = unicode(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        return words\n",
    "\n",
    "    def tokenize_random_tweet(self):\n",
    "        \"\"\"\n",
    "        If the twitter library is installed and a twitter connection\n",
    "        can be established, then tokenize a random tweet.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import twitter\n",
    "        except ImportError:\n",
    "            print \"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\"\n",
    "        from random import shuffle\n",
    "        api = twitter.Api()\n",
    "        tweets = api.GetPublicTimeline()\n",
    "        if tweets:\n",
    "            for tweet in tweets:\n",
    "                if tweet.user.lang == 'en':            \n",
    "                    return self.tokenize(tweet.text)\n",
    "        else:\n",
    "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))  \n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "def get_rel_popularity(c_k, c_all):\n",
    "    return log(1.0 * c_k / c_all) / log(2)\n",
    "\n",
    "\n",
    "def print_tokens(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print '=' * 5 + ' ' + group_name + ' ' + '=' * 5\n",
    "    for t, n in tokens:\n",
    "        print \"%s\\t%.4f\" % (t, n)\n",
    "    print\n",
    "\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "def print_count(rdd):\n",
    "    print 'Number of elements:', rdd.count()\n",
    "\n",
    "def safe_parse(raw_json):\n",
    "    # your code here\n",
    "    try:\n",
    "        json_object = json.loads(raw_json)\n",
    "    except ValueError, e: # this outputs None\n",
    "        pass\n",
    "    if 'user' in json_object and 'text' in json_object and 'recipient' not in json_object:\n",
    "        return (json_object['user']['id_str'],json_object['text'].encode('utf-8'))\n",
    "        \n",
    "    \n",
    "    return None #return False\n",
    "\n",
    "\n",
    "def print_users_count(count):\n",
    "    print 'The number of unique users is:', count\n",
    "\n",
    "def print_post_count(counts):\n",
    "    for group_id, count in counts:\n",
    "        print 'Group %d posted %d tweets' % (group_id, count)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#../../Data/hw6-files-final.txt\n",
    "#../../Data/hw6-files-4gb.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tweet_tuple = rdd.map(lambda x: safe_parse(x)).filter(lambda x: x != None).cache()\n",
    "count = tweet_tuple.keys().distinct().count() # this is correct if safe_parse returns a tuple\n",
    "\n",
    "print_users_count(count)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "#group = tweet_tuple.map(lambda x: (pd[x[0]], 1) if x[0]  in pd else (7,1) ).reduceByKey(lambda x,y: x+y).sortByKey().collect()\n",
    "#        group = tweet_tuple.flatMap(lambda x: [pd[x[0]]] if x[0]  in pd else [7] ).countByValue()\n",
    "\n",
    "\n",
    "#         group = tweet_tuple.flatMap(lambda x: [pd[x[0]]] if x[0]  in pd else [7] ).countByValue()\n",
    "#         print_post_count(group.items())\n",
    "        #tweet_map = tweet_tuple.keys().collect()\n",
    "#----------- this worksbest so far -----------------\n",
    "\n",
    "\n",
    "                # tweet_map = tweet_tuple.keys().collect()\n",
    "                #     #tweet_groups = set(tweet_map).intersection(set(pd.keys()))\n",
    "                # #tweet_groups = tweet_map&pd.viewkeys()\n",
    "                #         #tweet_group_2 = filter(lambda x: x in pd, tweet_map) # this gives correct number for group 0-6\n",
    "                # tweet_group_3 = map(lambda x: [pd[x]] if x in pd else [7], tweet_map) #gives correct number for group0-7\n",
    "                # group =  map( lambda i: (i,len(filter(lambda x: i in x, tweet_group_3)) ) ,range(8)  )\n",
    "\n",
    "#-----------------------------------------\n",
    "#print len(tweet_group_3)\n",
    "        #group_count= {}\n",
    "#y = filter(lambda x:dict[x] > 0.0,dict.keys())\n",
    "        # group_count[7] = len(tweet_groups)#len(set(pd)) - len(tweet_groups)\n",
    "        # print group_count\n",
    "# for i in tweet_groups:\n",
    "#     group_count[pd[i]] = (group_count[pd[i]]+1)\n",
    "\n",
    "#group = {k:pd[k] for k in tweet_groups}\n",
    "#print group\n",
    "# PRINT SIZE OF DIFF SETS BASED ON GROUPS\n",
    "# print type(pd)\n",
    "# pd_rdd= sc.parallelize(list(pd))\n",
    "# print pd_rdd.take(2)\n",
    "# group = tweet_tuple.join(pd_rdd)\n",
    "# print pd_rdd.flatMap(lambda x: x.items()).take(2)\n",
    "# print group.take(2)\n",
    "\n",
    "# print_post_count(group.collect())#.items())\n",
    "\n",
    "pd_rdd = sc.parallelize(pd.items()).cache()\n",
    "group = tweet_tuple.join(pd_rdd)\n",
    "group_0_6 = group.flatMap(lambda x: [x[1][1]]).countByValue()\n",
    "group_7 = tweet_tuple.subtractByKey(group).flatMap(lambda x: [7]).countByValue()\n",
    "print_post_count(group_0_6.items() + group_7.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "user_tweet_tok = tweet_tuple.map(lambda x: (x[0],tok.tokenize(x[1]))).reduceByKey(lambda x,y: x+y)# group all tweets of a user, thre are duplicate tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#---- new tokens ----------------\n",
    "# tokens = user_tweet_tok.map(lambda x: (x[0],list(set(x[1])))).cache()# get rid of duplicate tokens per user and combine all users into 1 list\n",
    "# print_count(tokens.flatMap(lambda x: x[1]).distinct()) \n",
    "\n",
    "#--------- old tokens -----------------\n",
    "tokens = user_tweet_tok.map(lambda x: (x[0],list(set(x[1]))))#.cache()\n",
    "flat_tok = tokens.flatMap(lambda x: x[1])#.cache()\n",
    "distinct_tok = flat_tok.distinct()# gets distinct tokens over all users  #.reduce(lambda x, y: print_count(x)+ print_count(y))\n",
    "print_count(distinct_tok)\n",
    "\n",
    "\n",
    "#----------- old tok_used_100 ----------------\n",
    "tok_used_100 = flat_tok.map(lambda x: (x,1))\\\n",
    "                        .reduceByKey(lambda x,y: x+y)\\\n",
    "                        .filter(lambda x: x[1] >= 100).cache()\n",
    "\n",
    "#------------- new tok_used_100                    \n",
    "# tok_used_100 = tokens.mapValues(lambda x: map((lambda m: (m,1)) ,x))\\\n",
    "#                     .flatMap(lambda x: x[1])\\\n",
    "#                     .reduceByKey(lambda x,y: x+y)\\\n",
    "#                     .filter(lambda x: x[1] >= 100).cache()\n",
    "#[(u'xa6', 457), (u'rt', 1237)]\n",
    "#-------------- test    tok_used_100 ---------------------\n",
    "# tok_used_100 = tokens.flatMapValues(lambda x: map((lambda m: [(m,1)]) ,x))\\\n",
    "#                     .reduceByKey(lambda x,y: x+y)\\\n",
    "#                     .filter(lambda x: x[1] >= 100)#.cache()\n",
    "        #[(u'2725924351', [(u'people', 1), (u'is', 1), (u'rt', 1)       \n",
    "#print tok_used_100.take(2)\n",
    "\n",
    "print_count(tok_used_100)\n",
    "print_tokens(tok_used_100.takeOrdered(20, lambda x: -x[1]))   #.take(20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "#grouped_users = tokens.map(lambda x: (pd[x[0]], x[1]) if x[0]  in pd else (7,x[1]) ).reduceByKey(lambda x,y: x+y).cache()\n",
    "\n",
    "#------------- new -----------------\n",
    "    # token_map = tokens.collectAsMap()\n",
    "    # tok_group = map(lambda x: (pd[x],token_map[x]) if x in pd else (7, token_map[x] ), token_map)#gives correct number for group0-7\n",
    "    # grouped_users_2 = sc.parallelize(tok_group).reduceByKey(lambda x,y: x+y).cache()\n",
    "#--------------------------\n",
    "\n",
    "#grouped_users_2 = \n",
    "\n",
    "#group =  map( lambda i: (i,len(filter(lambda x: i in x, tweet_group_3)) ) ,range(8)  )\n",
    "\n",
    "# print grouped_users.take(1)\n",
    "# print \"----------------------------------------------------\"\n",
    "# print grouped_users_2.take(1)\n",
    "\n",
    "#----------- group 7 fixed---------------\n",
    "token_joined = tokens.join(pd_rdd)\n",
    "tok_group_0_6 = token_joined.map(lambda x: (x[1][1], x[1][0] )).reduceByKey(lambda x,y: x+y)\n",
    "tok_group_7 = tokens.subtractByKey(token_joined).map(lambda x: (7, x[1] )).reduceByKey(lambda x,y: x+y)\n",
    "grouped_users_3 = tok_group_0_6.union(tok_group_7).cache()#.reduceByKey(lambda x,y: x+y).cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "#     group_i = grouped_users.filter(lambda x: x[0] == i )\\\n",
    "#                 .flatMap(lambda x: x[1] )\\\n",
    "#                 .map(lambda x:(x,1))\\\n",
    "#                 .reduceByKey(lambda x,y: x+y)\\\n",
    "#                 .join(tok_used_100)\\\n",
    "#                 .mapValues(lambda x: get_rel_popularity (x[0],x[1]))\\\n",
    "#                 .takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "#     print_tokens(group_i, i)\n",
    "    \n",
    "    #------- new -------\n",
    "    group_i = grouped_users_3.filter(lambda x: x[0] == i )\\\n",
    "                    .flatMap(lambda x: map((lambda m: (m,1)) ,x[1]))\\\n",
    "                    .reduceByKey(lambda x,y: x+y)\\\n",
    "                    .join(tok_used_100)\\\n",
    "                    .mapValues(lambda x: get_rel_popularity (x[0],x[1]))\\\n",
    "                    .takeOrdered(10, lambda x: x[0] and -x[1])\n",
    "    print_tokens(group_i, i)\n",
    "    \n",
    "    \n",
    "    \n",
    "#--------  test -----------\n",
    "#     group_i = grouped_users_2.filter(lambda x: x[0] == i )\\\n",
    "#                     .flatMap(lambda x: map((lambda m: (m,1)) ,x[1]))\\\n",
    "#                     .reduceByKey(lambda x,y: x+y)\\\n",
    "#                     .join(tok_used_100)\\\n",
    "#                     .mapValues(lambda x: get_rel_popularity (x[0],x[1]))\\\n",
    "#                     .takeOrdered(10, lambda x: x[0] and -x[1])\n",
    "#     print_tokens(group_i, i)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------ BAD -------\n",
    "#     group_i = grouped_users.filter(lambda x: x[0] == i ).values().flatMapValues(lambda x:(x,1))#.reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "#     print group_i.take(2)\n",
    "#     print_tokens(group_i, i)\n",
    "\n",
    "# group_0 = grouped_users.filter(lambda x: x[0] == 0 ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "# print_tokens(group_0, 0)\n",
    "\n",
    "# group_1 = grouped_users.filter(lambda x: x[0] == 1 ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "# print_tokens(group_1, 1)\n",
    "\n",
    "# group_2 = grouped_users.filter(lambda x: x[0] == 2 ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "# print_tokens(group_2, 2)\n",
    "\n",
    "# group_3 = grouped_users.filter(lambda x: x[0] == 3 ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "# print_tokens(group_3, 3)\n",
    "\n",
    "# group_4 = grouped_users.filter(lambda x: x[0] == 4 ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "# print_tokens(group_4, 4)\n",
    "\n",
    "# group_5 = grouped_users.filter(lambda x: x[0] == 5 ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "# print_tokens(group_5, 5)\n",
    "\n",
    "# group_6 = grouped_users.filter(lambda x: x[0] == 6 ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "# print_tokens(group_6, 6)\n",
    "\n",
    "# group_7 = grouped_users.filter(lambda x: x[0] == 7 ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "# print_tokens(group_7, 7)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "     #.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "    #print_tokens([grouped_users.filter(lambda x: x[0] == i ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])for i in range(8)], [0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()    # Remove the findspark lines later\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "def print_count(rdd):\n",
    "    print 'Number of elements:', rdd.count()\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#../../Data/hw6-files-final.txt\n",
    "#../../Data/hw6-files-4gb.txt\n",
    "\n",
    "with open(\"../../Data/hw6-files-10mb.txt\",\"r\") as f:\n",
    "    line = map(lambda x: x.strip(), f)\n",
    "    rdd = sc.textFile(','.join(line)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements: 2193\n",
      "The number of unique users is: 2083\n",
      "Group 0 posted 81 tweets\n",
      "Group 1 posted 199 tweets\n",
      "Group 2 posted 45 tweets\n",
      "Group 3 posted 313 tweets\n",
      "Group 4 posted 86 tweets\n",
      "Group 5 posted 221 tweets\n",
      "Group 6 posted 400 tweets\n",
      "Group 7 posted 798 tweets\n",
      "Number of elements: 8979\n",
      "Number of elements: 52\n",
      "===== overall =====\n",
      ":\t1386.0000\n",
      "rt\t1237.0000\n",
      ".\t865.0000\n",
      "\\\t745.0000\n",
      "the\t621.0000\n",
      "trump\t595.0000\n",
      "x80\t545.0000\n",
      "xe2\t543.0000\n",
      "to\t499.0000\n",
      ",\t489.0000\n",
      "xa6\t457.0000\n",
      "a\t403.0000\n",
      "is\t376.0000\n",
      "in\t296.0000\n",
      "'\t294.0000\n",
      "of\t292.0000\n",
      "and\t287.0000\n",
      "for\t280.0000\n",
      "!\t269.0000\n",
      "?\t210.0000\n",
      "\n",
      "===== group 0 =====\n",
      "...\t-3.5648\n",
      "at\t-3.5983\n",
      "hillary\t-4.0484\n",
      "bernie\t-4.1430\n",
      "not\t-4.2479\n",
      "he\t-4.2574\n",
      "i\t-4.2854\n",
      "s\t-4.3309\n",
      "are\t-4.3646\n",
      "in\t-4.4021\n",
      "\n",
      "===== group 1 =====\n",
      "#demdebate\t-2.4391\n",
      "-\t-2.6202\n",
      "clinton\t-2.7174\n",
      "&\t-2.7472\n",
      "amp\t-2.7472\n",
      ";\t-2.7980\n",
      "sanders\t-2.8745\n",
      "?\t-2.9069\n",
      "in\t-2.9615\n",
      "if\t-2.9861\n",
      "\n",
      "===== group 2 =====\n",
      "are\t-4.6865\n",
      "and\t-4.7055\n",
      "bernie\t-4.7279\n",
      "at\t-4.7682\n",
      "sanders\t-4.9449\n",
      "in\t-5.0395\n",
      "donald\t-5.0531\n",
      "a\t-5.0697\n",
      "#demdebate\t-5.1396\n",
      "that\t-5.1599\n",
      "\n",
      "===== group 3 =====\n",
      "#demdebate\t-1.3847\n",
      "bernie\t-1.8535\n",
      "sanders\t-2.1793\n",
      "of\t-2.2356\n",
      "t\t-2.2675\n",
      "clinton\t-2.4179\n",
      "hillary\t-2.4203\n",
      "the\t-2.4330\n",
      "xa6\t-2.4962\n",
      "that\t-2.5160\n",
      "\n",
      "===== group 4 =====\n",
      "hillary\t-3.8074\n",
      "sanders\t-3.9449\n",
      "of\t-4.0199\n",
      "what\t-4.0875\n",
      "clinton\t-4.0959\n",
      "at\t-4.1832\n",
      "in\t-4.2095\n",
      "a\t-4.2623\n",
      "on\t-4.2854\n",
      "'\t-4.2928\n",
      "\n",
      "===== group 5 =====\n",
      "cruz\t-2.3344\n",
      "he\t-2.6724\n",
      "will\t-2.7705\n",
      "are\t-2.7796\n",
      "the\t-2.8522\n",
      "is\t-2.8822\n",
      "that\t-2.9119\n",
      "this\t-2.9542\n",
      "for\t-2.9594\n",
      "of\t-2.9804\n",
      "\n",
      "===== group 6 =====\n",
      "@realdonaldtrump\t-1.1520\n",
      "cruz\t-1.4657\n",
      "n\t-1.4877\n",
      "!\t-1.5479\n",
      "not\t-1.8904\n",
      "xa6\t-1.9172\n",
      "xe2\t-1.9973\n",
      "/\t-2.0238\n",
      "x80\t-2.0240\n",
      "will\t-2.0506\n",
      "\n",
      "===== group 7 =====\n",
      "donald\t-0.6471\n",
      "...\t-0.7922\n",
      "sanders\t-1.0380\n",
      "what\t-1.1178\n",
      "trump\t-1.1293\n",
      "bernie\t-1.2044\n",
      "you\t-1.2099\n",
      "-\t-1.2253\n",
      "if\t-1.2602\n",
      "clinton\t-1.2681\n",
      "\n",
      "CPU times: user 3.42 s, sys: 230 ms, total: 3.65 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# # Homework 6\n",
    "\n",
    "# Name: Michelle Pang\n",
    "# Email: mapang@eng.ucsd.edu\n",
    "# PID: A10660264\n",
    "\n",
    "\n",
    "# with open(\"../../Data/hw6-files-4gb.txt\",\"r\") as f:\n",
    "#    # lines = f.readlines()\n",
    "\n",
    "#     rdd = sc.union([sc.textFile( line.strip()) for line in f]).cache()  # with this change time is 132 sec\n",
    "print_count(rdd)\n",
    "\n",
    "\n",
    "# # Part 1: Parse JSON strings to JSON objects\n",
    "\n",
    "# Python has built-in support for JSON.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import json\n",
    "\n",
    "json_example = '''\n",
    "{\n",
    "    \"id\": 1,\n",
    "    \"name\": \"A green door\",\n",
    "    \"price\": 12.50,\n",
    "    \"tags\": [\"home\", \"green\"]\n",
    "}\n",
    "'''\n",
    "\n",
    "json_obj = json.loads(json_example)\n",
    "json_obj\n",
    "\n",
    "\n",
    "# ## Broken tweets and irrelevant messages\n",
    "# \n",
    "# The data of this assignment may contain broken tweets (invalid JSON strings). So make sure that your code is robust for such cases.\n",
    "# \n",
    "# In addition, some lines in the input file might not be tweets, but messages that the Twitter server sent to the developer (such as [limit notices](https://dev.twitter.com/streaming/overview/messages-types#limit_notices)). Your program should also ignore these messages.\n",
    "# \n",
    "# *Hint:* [Catch the ValueError](http://stackoverflow.com/questions/11294535/verify-if-a-string-is-json-in-python)\n",
    "# \n",
    "# \n",
    "# (1) Parse raw JSON tweets to obtain valid JSON objects. From all valid tweets, construct a pair RDD of `(user_id, text)`, where `user_id` is the `id_str` data field of the `user` dictionary (read [Tweets](#Tweets) section above), `text` is the `text` data field.\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "#import json\n",
    "\n",
    "def safe_parse(raw_json):\n",
    "    # your code here\n",
    "    try:\n",
    "        json_object = json.loads(raw_json)\n",
    "    except ValueError, e: # this outputs None\n",
    "        pass\n",
    "    if 'user' in json_object and 'text' in json_object and 'recipient' not in json_object:\n",
    "        return (json_object['user']['id_str'],json_object['text'].encode('utf-8'))\n",
    "        \n",
    "    \n",
    "    return None #return False\n",
    "\n",
    "\n",
    "\n",
    "# (2) Count the number of different users in all valid tweets (hint: [the `distinct()` method](https://spark.apache.org/docs/latest/programming-guide.html#transformations)).\n",
    "# \n",
    "# It should print\n",
    "# ```\n",
    "# The number of unique users is: 2083\n",
    "# ```\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "def print_users_count(count):\n",
    "    print 'The number of unique users is:', count\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "# your code here\n",
    "\n",
    "tweet_tuple = rdd.map(lambda x: safe_parse(x)).filter(lambda x: x != None).cache()\n",
    "count = tweet_tuple.map(lambda x: x[0]).distinct().count() # this is correct if safe_parse returns a tuple\n",
    "\n",
    "print_users_count(count)\n",
    "\n",
    "\n",
    "# # Part 2: Number of posts from each user partition\n",
    "\n",
    "# Load the Pickle file `../../Data/users-partition.pickle`, you will get a dictionary which represents a partition over 452,743 Twitter users, `{user_id: partition_id}`. The users are partitioned into 7 groups. For example, if the dictionary is loaded into a variable named `partition`, the partition ID of the user `59458445` is `partition[\"59458445\"]`. These users are partitioned into 7 groups. The partition ID is an integer between 0-6.\n",
    "# \n",
    "# Note that the user partition we provide doesn't cover all users appear in the input data.\n",
    "\n",
    "# (1) Load the pickle file.\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "# your code here\n",
    "\n",
    "import pickle\n",
    "filename = \"../../Data/users-partition.pickle\"\n",
    "pd=pickle.load(open(filename,'rb'))\n",
    "\n",
    "\n",
    "\n",
    "# (2) Count the number of posts from each user partition\n",
    "# \n",
    "# Count the number of posts from group 0, 1, ..., 6, plus the number of posts from users who are not in any partition. Assign users who are not in any partition to the group 7.\n",
    "# \n",
    "# Put the results of this step into a pair RDD `(group_id, count)` that is sorted by key.\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "# your code here\n",
    "#--- opt 1\n",
    "#group = tweet_tuple.map(lambda x: (pd[x[0]], 1) if x[0]  in pd else (7,1) ).reduceByKey(lambda x,y: x+y).sortByKey().collect()\n",
    "#---opt 2\n",
    "#group = tweet_tuple.flatMap(lambda x: [pd[x[0]]] if x[0]  in pd else [7] ).countByValue()\n",
    "#---opt 3\n",
    "        # tweet_map = tweet_tuple.keys().collect()\n",
    "        # tweet_group_3 = map(lambda x: [pd[x]] if x in pd else [7], tweet_map) #gives correct number for group0-7\n",
    "        # group =  map( lambda i: (i,len(filter(lambda x: i in x, tweet_group_3)) ) ,range(8)  )\n",
    "#---opt 4\n",
    "pd_rdd = sc.parallelize(pd.items()).cache()\n",
    "group = tweet_tuple.join(pd_rdd)\n",
    "group_0_6 = group.flatMap(lambda x: [x[1][1]]).countByValue()\n",
    "group_7 = tweet_tuple.subtractByKey(group).flatMap(lambda x: [7]).countByValue()\n",
    "\n",
    "\n",
    "# (3) Print the post count using the `print_post_count` function we provided.\n",
    "# \n",
    "# It should print\n",
    "# \n",
    "# ```\n",
    "# Group 0 posted 81 tweets\n",
    "# Group 1 posted 199 tweets\n",
    "# Group 2 posted 45 tweets\n",
    "# Group 3 posted 313 tweets\n",
    "# Group 4 posted 86 tweets\n",
    "# Group 5 posted 221 tweets\n",
    "# Group 6 posted 400 tweets\n",
    "# Group 7 posted 798 tweets\n",
    "# ```\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "def print_post_count(counts):\n",
    "    for group_id, count in counts:\n",
    "        print 'Group %d posted %d tweets' % (group_id, count)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "# your code here\n",
    "\n",
    "#print_post_count(group)#.items()) #group) #group.items())\n",
    "print_post_count(group_0_6.items() + group_7.items())\n",
    "\n",
    "# # Part 3:  Tokens that are relatively popular in each user partition\n",
    "\n",
    "# In this step, we are going to find tokens that are relatively popular in each user partition.\n",
    "# \n",
    "# We define the number of mentions of a token $t$ in a specific user partition $k$ as the number of users from the user partition $k$ that ever mentioned the token $t$ in their tweets. Note that even if some users might mention a token $t$ multiple times or in multiple tweets, a user will contribute at most 1 to the counter of the token $t$.\n",
    "# \n",
    "# Please make sure that the number of mentions of a token is equal to the number of users who mentioned this token but NOT the number of tweets that mentioned this token.\n",
    "# \n",
    "# Let $N_t^k$ be the number of mentions of the token $t$ in the user partition $k$. Let $N_t^{all} = \\sum_{i=0}^7 N_t^{i}$ be the number of total mentions of the token $t$.\n",
    "# \n",
    "# We define the relative popularity of a token $t$ in a user partition $k$ as the log ratio between $N_t^k$ and $N_t^{all}$, i.e. \n",
    "# \n",
    "# \\begin{equation}\n",
    "# p_t^k = \\log \\frac{C_t^k}{C_t^{all}}.\n",
    "# \\end{equation}\n",
    "# \n",
    "# \n",
    "# You can compute the relative popularity by calling the function `get_rel_popularity`.\n",
    "\n",
    "# (0) Load the tweet tokenizer.\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "# %load happyfuntokenizing.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\n",
    "\n",
    "Julaiti Alafate:\n",
    "  I modified the regex strings to extract URLs in tweets.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Christopher Potts\"\n",
    "__copyright__ = \"Copyright 2011, Christopher Potts\"\n",
    "__credits__ = []\n",
    "__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-nc-sa/3.0/\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Christopher Potts\"\n",
    "__email__ = \"See the author's website\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "import htmlentitydefs\n",
    "\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # URLs:\n",
    "    r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # HTML tags:\n",
    "     r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False):\n",
    "        self.preserve_case = preserve_case\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        try:\n",
    "            s = unicode(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = str(s).encode('string_escape')\n",
    "            s = unicode(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        return words\n",
    "\n",
    "    def tokenize_random_tweet(self):\n",
    "        \"\"\"\n",
    "        If the twitter library is installed and a twitter connection\n",
    "        can be established, then tokenize a random tweet.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import twitter\n",
    "        except ImportError:\n",
    "            print \"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\"\n",
    "        from random import shuffle\n",
    "        api = twitter.Api()\n",
    "        tweets = api.GetPublicTimeline()\n",
    "        if tweets:\n",
    "            for tweet in tweets:\n",
    "                if tweet.user.lang == 'en':            \n",
    "                    return self.tokenize(tweet.text)\n",
    "        else:\n",
    "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "def get_rel_popularity(c_k, c_all):\n",
    "    return log(1.0 * c_k / c_all) / log(2)\n",
    "\n",
    "\n",
    "def print_tokens(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print '=' * 5 + ' ' + group_name + ' ' + '=' * 5\n",
    "    for t, n in tokens:\n",
    "        print \"%s\\t%.4f\" % (t, n)\n",
    "    print\n",
    "\n",
    "\n",
    "# (1) Tokenize the tweets using the tokenizer we provided above named `tok`. Count the number of mentions for each tokens regardless of specific user group.\n",
    "# \n",
    "# Call `print_count` function to show how many different tokens we have.\n",
    "# \n",
    "# It should print\n",
    "# ```\n",
    "# Number of elements: 8949\n",
    "# ```\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "user_tweet_tok = tweet_tuple.map(lambda x: (x[0],tok.tokenize(x[1]))).reduceByKey(lambda x,y: x+y)# group all tweets of a user, thre are duplicate tokens\n",
    "\n",
    "\n",
    "#---- pairs it up as (user, user's tokens)\n",
    "tokens = user_tweet_tok.map(lambda x: (x[0],list(set(x[1]))))#.cache()# get rid of duplicate tokens per user and combine all users into 1 list\n",
    "flat_tok = tokens.flatMap(lambda x: x[1])#.cache()\n",
    "distinct_tok = flat_tok.distinct()# gets distinct tokens over all users  #.reduce(lambda x, y: print_count(x)+ print_count(y))\n",
    "\n",
    "\n",
    "\n",
    "print_count(distinct_tok)\n",
    "\n",
    "\n",
    "# (2) Tokens that are mentioned by too few users are usually not very interesting. So we want to only keep tokens that are mentioned by at least 100 users. Please filter out tokens that don't meet this requirement.\n",
    "# \n",
    "# Call `print_count` function to show how many different tokens we have after the filtering.\n",
    "# \n",
    "# Call `print_tokens` function to show top 20 most frequent tokens.\n",
    "# \n",
    "# It should print\n",
    "# ```\n",
    "# Number of elements: 44\n",
    "# ===== overall =====\n",
    "# :\t1388.0000\n",
    "# rt\t1237.0000\n",
    "# .\t826.0000\n",
    "# …\t673.0000\n",
    "# the\t623.0000\n",
    "# trump\t582.0000\n",
    "# to\t499.0000\n",
    "# ,\t489.0000\n",
    "# a\t404.0000\n",
    "# is\t376.0000\n",
    "# in\t297.0000\n",
    "# of\t292.0000\n",
    "# and\t288.0000\n",
    "# for\t281.0000\n",
    "# !\t269.0000\n",
    "# ?\t210.0000\n",
    "# on\t195.0000\n",
    "# i\t192.0000\n",
    "# you\t191.0000\n",
    "# this\t190.0000\n",
    "# ```\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "tok_used_100 = flat_tok.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] >= 100).cache()#.sortBy(lambda x: x[1], False).cache()#.map(lambda (c,v): (v,c)).sortByKey(False).map(lambda (c,v): (v,c))\n",
    "\n",
    "print_count(tok_used_100)\n",
    "print_tokens(tok_used_100.takeOrdered(20, lambda x: -x[1]))   #.take(20))\n",
    "\n",
    "\n",
    "# (3) For all tokens that are mentioned by at least 100 users, compute their relative popularity in each user group. Then print the top 10 tokens with highest relative popularity in each user group. In case two tokens have same relative popularity, break the tie by printing the alphabetically smaller one.\n",
    "# \n",
    "# **Hint:** Let the relative popularity of a token $t$ be $p$. The order of the items will be satisfied by sorting them using (-p, t) as the key.\n",
    "# \n",
    "# It should print\n",
    "# ```\n",
    "# ===== group 0 =====\n",
    "# ...\t-3.5648\n",
    "# at\t-3.5983\n",
    "# hillary\t-4.0875\n",
    "# i\t-4.1255\n",
    "# bernie\t-4.1699\n",
    "# not\t-4.2479\n",
    "# https\t-4.2695\n",
    "# he\t-4.2801\n",
    "# in\t-4.3074\n",
    "# are\t-4.3646\n",
    "# \n",
    "# ===== group 1 =====\n",
    "# #demdebate\t-2.4391\n",
    "# -\t-2.6202\n",
    "# &\t-2.7472\n",
    "# amp\t-2.7472\n",
    "# clinton\t-2.7570\n",
    "# ;\t-2.7980\n",
    "# sanders\t-2.8838\n",
    "# ?\t-2.9069\n",
    "# in\t-2.9664\n",
    "# if\t-3.0138\n",
    "# \n",
    "# ===== group 2 =====\n",
    "# are\t-4.6865\n",
    "# and\t-4.7105\n",
    "# bernie\t-4.7549\n",
    "# at\t-4.7682\n",
    "# sanders\t-4.9542\n",
    "# that\t-5.0224\n",
    "# in\t-5.0444\n",
    "# donald\t-5.0618\n",
    "# a\t-5.0732\n",
    "# #demdebate\t-5.1396\n",
    "# \n",
    "# ===== group 3 =====\n",
    "# #demdebate\t-1.3847\n",
    "# bernie\t-1.8480\n",
    "# sanders\t-2.1887\n",
    "# of\t-2.2356\n",
    "# that\t-2.3785\n",
    "# the\t-2.4376\n",
    "# …\t-2.4403\n",
    "# clinton\t-2.4467\n",
    "# hillary\t-2.4594\n",
    "# be\t-2.5465\n",
    "# \n",
    "# ===== group 4 =====\n",
    "# hillary\t-3.7395\n",
    "# sanders\t-3.9542\n",
    "# of\t-4.0199\n",
    "# clinton\t-4.0790\n",
    "# at\t-4.1832\n",
    "# in\t-4.2143\n",
    "# a\t-4.2659\n",
    "# on\t-4.2854\n",
    "# .\t-4.3681\n",
    "# the\t-4.4251\n",
    "# \n",
    "# ===== group 5 =====\n",
    "# cruz\t-2.3861\n",
    "# he\t-2.6280\n",
    "# are\t-2.7796\n",
    "# will\t-2.7829\n",
    "# the\t-2.8568\n",
    "# is\t-2.8822\n",
    "# for\t-2.9250\n",
    "# that\t-2.9349\n",
    "# of\t-2.9804\n",
    "# this\t-2.9849\n",
    "# \n",
    "# ===== group 6 =====\n",
    "# @realdonaldtrump\t-1.1520\n",
    "# cruz\t-1.4532\n",
    "# https\t-1.5222\n",
    "# !\t-1.5479\n",
    "# not\t-1.8904\n",
    "# …\t-1.9269\n",
    "# will\t-2.0124\n",
    "# it\t-2.0345\n",
    "# this\t-2.1104\n",
    "# to\t-2.1685\n",
    "# \n",
    "# ===== group 7 =====\n",
    "# donald\t-0.6422\n",
    "# ...\t-0.7922\n",
    "# sanders\t-1.0282\n",
    "# trump\t-1.1296\n",
    "# bernie\t-1.2106\n",
    "# -\t-1.2253\n",
    "# you\t-1.2376\n",
    "# clinton\t-1.2511\n",
    "# if\t-1.2880\n",
    "# i\t-1.2996\n",
    "# ```\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "# your code here\n",
    "\n",
    "#-- opt 1\n",
    "        #grouped_users = tokens.map(lambda x: (pd[x[0]], x[1]) if x[0]  in pd else (7,x[1]) ).reduceByKey(lambda x,y: x+y)#.cache()#.sortByKey().collect()\n",
    "#-- opt 2\n",
    "    #grouped_users_count = grouped_users.mapValues(lambda x: map ((lambda y: (y,1) ), x)).cache()\n",
    "           \n",
    "#-- opt 3       \n",
    "        # token_map = tokens.collectAsMap()\n",
    "            # tok_group = map(lambda x: (pd[x],token_map[x]) if x in pd else (7, token_map[x] ), token_map)#gives correct number for group0-7\n",
    "            # grouped_users = sc.parallelize(tok_group).reduceByKey(lambda x,y: x+y).cache()\n",
    "#-- opt 4\n",
    "token_joined = tokens.join(pd_rdd)\n",
    "tok_group_0_6 = token_joined.map(lambda x: (x[1][1], x[1][0] )).reduceByKey(lambda x,y: x+y)\n",
    "tok_group_7 = tokens.subtractByKey(token_joined).map(lambda x: (7, x[1] )).reduceByKey(lambda x,y: x+y)\n",
    "grouped_users_3 = tok_group_0_6.union(tok_group_7).cache()\n",
    "\n",
    "#---------- old option -------------------\n",
    "# for i in range(8):\n",
    "#     group_i = grouped_users.mapValues(lambda x: map ((lambda y: (y,1) ), x)).filter(lambda x: x[0] == i ).flatMap(lambda x: x[1] ).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).sortByKey(True).sortBy(lambda x: x[1], False).take(10)\n",
    "#     print_tokens(group_i, i)\n",
    "\n",
    "#-------- new ----------------------\n",
    "# for i in range(8):\n",
    "#     group_i = grouped_users.filter(lambda x: x[0] == i ).flatMap(lambda x: x[1] ).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).join(tok_used_100).mapValues(lambda x: get_rel_popularity (x[0],x[1])).takeOrdered(10, lambda x: x[0] and -x[1])#.sortByKey(True).takeOrdered(10, lambda x: -x[1])#.sortBy(lambda x: x[1], False).take(10)\n",
    "#     print_tokens(group_i, i)\n",
    "\n",
    "#---------newer --------------------\n",
    "        # for i in range(8):\n",
    "        #     group_i = grouped_users.filter(lambda x: x[0] == i )\\\n",
    "        #                     .flatMap(lambda x: map((lambda m: (m,1)) ,x[1]))\\\n",
    "        #                     .reduceByKey(lambda x,y: x+y)\\\n",
    "        #                     .join(tok_used_100)\\\n",
    "        #                     .mapValues(lambda x: get_rel_popularity (x[0],x[1]))\\\n",
    "        #                     .takeOrdered(10, lambda x: x[0] and -x[1])\n",
    "        #     print_tokens(group_i, i)\n",
    "        \n",
    "#--------- test ---------------\n",
    "for i in range(8):\n",
    "    group_i = grouped_users_3.filter(lambda x: x[0] == i )\\\n",
    "                    .flatMap(lambda x: map((lambda m: (m,1)) ,x[1]))\\\n",
    "                    .reduceByKey(lambda x,y: x+y)\\\n",
    "                    .join(tok_used_100)\\\n",
    "                    .mapValues(lambda x: get_rel_popularity (x[0],x[1]))\\\n",
    "                    .takeOrdered(10, lambda x: x[0] and -x[1])\n",
    "    print_tokens(group_i, i)\n",
    "\n",
    "# (4) (optional, not for grading) The users partition is generated by a machine learning algorithm that tries to group the users by their political preferences. Three of the user groups are showing supports to Bernie Sanders, Ted Cruz, and Donald Trump. \n",
    "# \n",
    "# If your program looks okay on the local test data, you can try it on the larger input by submitting your program to the homework server. Observe the output of your program to larger input files, can you guess the partition IDs of the three groups mentioned above based on your output?\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Change the values of the following three items to your guesses\n",
    "# users_support = [\n",
    "#     (-1, \"Bernie Sanders\"),\n",
    "#     (-1, \"Ted Cruz\"),\n",
    "#     (-1, \"Donald Trump\")\n",
    "# ]\n",
    "\n",
    "# for gid, candidate in users_support:\n",
    "#     print \"Users from group %d are most likely to support %s.\" % (gid, candidate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
